{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fcbf81",
   "metadata": {},
   "source": [
    "## Technical Questions\n",
    "\n",
    "1. Data Modeling and Warehousing\n",
    "Question: How do you design a schema for a new data-intensive application?\n",
    "To design a schema for a new data-intensive application, I would first need to understand the business requirements and use cases. Then I would analyze the expected data types, relationships and queries. With that information, I would choose an appropriate data model, such as relational, NoSQL, etc. I would also consider requirements such as scalability, performance and cost.\n",
    "\n",
    "2. ETL/ELT Design and Implementation\n",
    "Question: Describe a complex ETL pipeline you've designed. What were some of the challenges, and how did you address them?\n",
    "I created an ETL with Python (Pandas, Colab) to generate a critical report for the company from lots of files about project initiatives with a level of risk for the company from all the countries where the company was located. Due to that wasn't measured and visible, the company had to pay a lot of money in fines, so I made the company save a lot of money. It was a challenge because I only had a free layer on Google Colab, so I had to optimize the code in order to meet the requirement successfully.\n",
    "\n",
    "3. Data Infrastructure and Orchestration\n",
    "Question: What experience do you have with managing data infrastructure on-premises or in the cloud?\n",
    "I have experience managing data infrastructure both on-premises and in the cloud. On-premises, I've worked with traditional database servers, data warehouses, etc.. In the cloud, I have experience with AWS and Google Cloud Platform services, such as S3 for storage, BigQuery for databases, as well as NoSQL DataBases. However, I consider myself to have basic experience regarding infrastructure matters, In spite of this, I am a quick really good learner, I love learn new things and apply immediately.\n",
    "\n",
    "4. Programming and Software Engineering Practices\n",
    "Question: What programming languages are you most comfortable with, and what libraries do you frequently use in data engineering projects?\n",
    "I am most comfortable with Python and SQL for data engineering projects. In Python, I frequently use libraries such as Pandas for data manipulation, as well as Numpy. I have worked with other programming languages such as Java, C#, Progress 4GL, however Python is my main language.\n",
    "\n",
    "5. Data Security and Compliance\n",
    "Question: How do you implement security measures in your data engineering projects?\n",
    "To implement security measures, it is important to follow best practices such as encryption of data in transit and at rest, role-based access control, secure authentication, and secure authentication. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bed726",
   "metadata": {},
   "source": [
    "Please explain how you ussually follow CI/CD pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e580798",
   "metadata": {},
   "source": [
    "I follow CI/CD pipelines by first setting up a version control system, such as Git, to manage code changes. I use tools like Jenkins, GitLab CI, or CircleCI to automate the build, test, and deployment processes. The pipeline typically includes steps for code linting, unit testing, integration testing, and deployment to staging and production environments. I use Docker for containerization and Kubernetes for orchestration to ensure consistent environments across development and production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967bcf82",
   "metadata": {},
   "source": [
    "## Code Challenge SQL\n",
    "\n",
    "Code Challenge Description\n",
    "Title: Building a MySQL Database Interface in Python\n",
    "\n",
    "Objective:\n",
    "You are tasked with creating a Python application that interfaces with a MySQL database. The application will manage a dataset representing sales data for a tech company that sells various products across multiple countries. Your goal is to establish a database connection, create a table, and populate this table with sample data.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Create a Database Connection:\n",
    "Implement a Python function to establish a connection to a MySQL database using provided credentials (host, username, password, and database name).\n",
    "Define and Create a Table:\n",
    "Write SQL commands within your Python script to create a table named sales. This table should have columns for id, country, category, price, quantity, and final_sales, with appropriate data types.\n",
    "Insert Data:\n",
    "Prepare a series of SQL INSERT statements to populate the sales table with the provided sample data. Ensure each record accurately reflects the sales data format.\n",
    "Execute Queries:\n",
    "Write functions to execute SQL queries to create the table and insert data into the table. Include error handling to manage potential SQL execution errors.\n",
    "Expected Deliverables:\n",
    "\n",
    "A Python script that can be run to connect to a MySQL database, create the necessary table, and populate it with data.\n",
    "Your script should handle common errors that might occur during database operations, such as connection failures or SQL syntax errors.\n",
    "Evaluation Criteria:\n",
    "\n",
    "Correctness: The script should correctly execute all database operations without errors.\n",
    "\n",
    "Code Quality: Code should be clear, well-organized, and appropriately commented.\n",
    "\n",
    "Error Handling: The script should effectively handle and report errors during database operations.\n",
    "\n",
    "Efficiency: SQL operations should be written efficiently to optimize execution.\n",
    "\n",
    "Setup Instructions\n",
    "Just use mysql local community server and a made up data set related to sales of devices in a tech company \n",
    "https://dev.mysql.com/downloads/mysql/\n",
    "\n",
    "Table 1 Sales \n",
    "    product_id ,\n",
    "    country ,\n",
    "    category ,\n",
    "    price ,\n",
    "    quantity ,\n",
    "    final_sales\n",
    "    \n",
    "Table 2 Product\n",
    "    id  PRIMARY KEY,\n",
    "    category ,\n",
    "    capacity ,\n",
    "    color ,\n",
    "    screen_size ,\n",
    "    memory ,\n",
    "    other_specs .\n",
    "Ensure you have mysql-connector-python installed in your environment. If not, you can install it using pip install mysql-connector-python.\n",
    "\n",
    "Tips for Success\n",
    "Test each part of your script incrementally to ensure that each function behaves as expected.\n",
    "Consider the edge cases, such as what happens if the table already exists or the database connection cannot be established.\n",
    "\n",
    "This challenge is designed to test your ability to integrate Python programming with SQL database management, reflecting tasks you may handle as a data engineer in our organization. Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bcd46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8118b",
   "metadata": {},
   "source": [
    "## Create a MySQL Database using Python\n",
    "### First, you need to connect to your MySQL server and create a new database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "# Function to establish database connection\n",
    "def create_connection(host_name, user_name, user_password, db_name):\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host_name,\n",
    "            user=user_name,\n",
    "            passwd=user_password,\n",
    "            database=db_name\n",
    "        )\n",
    "        print(\"Connection to MySQL DB successful\")\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "    return connection\n",
    "\n",
    "# Function to execute SQL queries\n",
    "def execute_query(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        connection.commit()\n",
    "        print(\"Query executed successfully\")\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "\n",
    "# Define the database credentials\n",
    "host = \"localhost\"\n",
    "user = \"root\"\n",
    "password = \"25789Mysql.\"\n",
    "database = \"sales_db\"\n",
    "\n",
    "# Create a database connection\n",
    "connection = create_connection(host, user, password, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13997158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the create table query\n",
    "create_sales_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sales (\n",
    "    id INT AUTO_INCREMENT,\n",
    "    country VARCHAR(255) NOT NULL,\n",
    "    category VARCHAR(255) NOT NULL,\n",
    "    price DECIMAL(10, 2) NOT NULL,\n",
    "    quantity INT NOT NULL,\n",
    "    final_sales DECIMAL(10, 2) NOT NULL,\n",
    "    PRIMARY KEY (id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the create table query\n",
    "execute_query(connection, create_sales_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84329b",
   "metadata": {},
   "source": [
    "## Insert Data into the sales Table\n",
    "### The following Python script shows how to insert data into the sales table. We'll be adding rows using a batch insert for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data to insert into the sales table\n",
    "sales_data = \"\"\"\n",
    "INSERT INTO sales (country, category, price, quantity, final_sales)\n",
    "VALUES\n",
    "  ('USA', 'Electronics', 249.99, 15, 3749.85),\n",
    "  ('Canada', 'Books', 19.95, 40, 798.00),\n",
    "  ('Germany', 'Clothing', 89.99, 22, 1979.78),\n",
    "  ('France', 'Home Decor', 59.95, 28, 1678.60),\n",
    "  ('Japan', 'Toys', 24.99, 35, 874.65),\n",
    "  ('Australia', 'Sports', 99.00, 18, 1782.00),\n",
    "  ('Brazil', 'Electronics', 179.99, 12, 2159.88),\n",
    "  ('Spain', 'Furniture', 399.00, 5, 1995.00),\n",
    "  ('China', 'Books', 9.99, 65, 649.35),\n",
    "  ('Italy', 'Clothing', 119.99, 16, 1919.84),\n",
    "  ('UK', 'Home Decor', 79.95, 20, 1599.00),\n",
    "  ('Mexico', 'Toys', 39.99, 30, 1199.70),\n",
    "  ('India', 'Sports', 49.99, 25, 1249.75),\n",
    "  ('Russia', 'Electronics', 299.99, 8, 2399.92),\n",
    "  ('South Africa', 'Furniture', 249.99, 7, 1749.93),\n",
    "  ('Turkey', 'Books', 14.99, 45, 674.55),\n",
    "  ('Saudi Arabia', 'Clothing', 99.99, 18, 1799.82),\n",
    "  ('Netherlands', 'Home Decor', 44.95, 32, 1438.40),\n",
    "  ('Belgium', 'Toys', 29.99, 40, 1199.60),\n",
    "  ('Switzerland', 'Electronics', 399.99, 6, 2399.94),\n",
    "  ('Sweden', 'Furniture', 299.99, 9, 2699.91),\n",
    "  ('Norway', 'Books', 24.95, 38, 948.10),\n",
    "  ('Denmark', 'Clothing', 79.99, 24, 1919.76),\n",
    "  ('Finland', 'Home Decor', 69.95, 26, 1818.70),\n",
    "  ('Poland', 'Sports', 89.99, 20, 1799.80);\n",
    "\"\"\"\n",
    "\n",
    "# Execute the insert data query\n",
    "execute_query(connection, sales_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b5b58",
   "metadata": {},
   "source": [
    "## Code to Create the product Table\n",
    "### First, here's the SQL command to create the product table with various specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_product_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS product (\n",
    "    product_id INT AUTO_INCREMENT,\n",
    "    product_name VARCHAR(255) NOT NULL,\n",
    "    category VARCHAR(255) NOT NULL,\n",
    "    price DECIMAL(10, 2) NOT NULL,\n",
    "    specifications TEXT NOT NULL,\n",
    "    country_of_origin VARCHAR(255) NOT NULL,\n",
    "    PRIMARY KEY (product_id)\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17bdc36",
   "metadata": {},
   "source": [
    "## Python Code to Execute the Table Creation and Insert Data\n",
    "### Now, let's integrate this into your Python script to create the table and then populate it with some sample data(please just create dummy data ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the create table query\n",
    "execute_query(connection, create_product_table)\n",
    "\n",
    "# Sample data to insert into the product table\n",
    "product_data = \"\"\"\n",
    "INSERT INTO product (product_name, category, price, specifications, country_of_origin)\n",
    "VALUES\n",
    "   ('Laptop', 'Electronics', 1000.00, '16GB RAM, 512GB SSD', 'USA'),\n",
    "   ('Sofa', 'Furniture', 500.00, 'Leather, Brown', 'Canada'),\n",
    "   ('Toy Car', 'Toys', 20.00, 'Plastic, Red', 'Germany'),\n",
    "   ('Smartphone', 'Electronics', 800.00, '6GB RAM, 128GB Storage', 'South Korea'),\n",
    "   ('Dining Table', 'Furniture', 300.00, 'Wood, Rectangular', 'Italy'),\n",
    "   ('Board Game', 'Toys', 30.00, 'Cardboard, Family-friendly', 'USA'),\n",
    "   ('Headphones', 'Electronics', 100.00, 'Noise-cancelling, Wireless', 'Japan'),\n",
    "   ('Armchair', 'Furniture', 200.00, 'Fabric, Blue', 'Spain'),\n",
    "   ('Puzzle', 'Toys', 15.00, '1000 pieces, Landscape', 'Germany'),\n",
    "   ('Tablet', 'Electronics', 600.00, '10.5-inch display, 64GB Storage', 'China'),\n",
    "   ('Coffee Table', 'Furniture', 150.00, 'Glass top, Metal frame', 'USA'),\n",
    "   ('Stuffed Animal', 'Toys', 10.00, 'Plush, Teddy Bear', 'Mexico'),\n",
    "   ('Television', 'Electronics', 1200.00, '55-inch, 4K Smart TV', 'South Korea'),\n",
    "   ('Bed Frame', 'Furniture', 400.00, 'Metal, Queen size', 'Canada'),\n",
    "   ('Action Figure', 'Toys', 25.00, 'Plastic, Superhero', 'USA'),\n",
    "   ('Laptop Bag', 'Electronics', 50.00, 'Nylon, Water-resistant', 'Taiwan'),\n",
    "   ('Dining Chairs', 'Furniture', 80.00, 'Wood, Set of 4', 'Italy'),\n",
    "   ('Lego Set', 'Toys', 40.00, '500 pieces, Space theme', 'Denmark'),\n",
    "   ('Smart Watch', 'Electronics', 300.00, 'Fitness tracking, Waterproof', 'USA'),\n",
    "   ('Bookshelf', 'Furniture', 120.00, 'Wood, 5 shelves', 'Poland'),\n",
    "   ('Dollhouse', 'Toys', 60.00, 'Wood, Furnished', 'Germany'),\n",
    "   ('Wireless Speaker', 'Electronics', 80.00, 'Bluetooth, Portable', 'China'),\n",
    "   ('Recliner', 'Furniture', 350.00, 'Leather, Power lift', 'Mexico'),\n",
    "   ('Drones', 'Toys', 100.00, 'Remote controlled, HD Camera', 'USA'),\n",
    "   ('Laptop Cooling Pad', 'Electronics', 30.00, 'Adjustable, USB-powered', 'Taiwan');\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6392abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the insert data query\n",
    "execute_query(connection, product_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e874fec",
   "metadata": {},
   "source": [
    "## Use Case: Detailed Sales Analysis\n",
    "### Objective:\n",
    "\n",
    "Determine the top-selling product categories in each country.\n",
    "Retrieve detailed product specifications for these top-selling products.\n",
    "Provide additional insights like the total number of distinct products sold and the maximum sales recorded for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform detailed sales analysis\n",
    "def detailed_sales_analysis(connection):\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        s.country,\n",
    "        s.category,\n",
    "        COUNT(DISTINCT p.product_id) AS distinct_products_sold,\n",
    "        MAX(s.final_sales) AS max_sales,\n",
    "        GROUP_CONCAT(p.product_name) AS product_names,\n",
    "        GROUP_CONCAT(p.specifications) AS specifications\n",
    "    FROM\n",
    "        sales s\n",
    "    JOIN\n",
    "        product p ON s.category = p.category\n",
    "    GROUP BY\n",
    "        s.country, s.category\n",
    "    ORDER BY\n",
    "        max_sales DESC;\n",
    "    \"\"\"\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "        for row in result:\n",
    "            print(f\"Country: {row[0]}, Category: {row[1]}, Distinct Products Sold: {row[2]}, Max Sales: {row[3]}, Product Names: {row[4]}, Specifications: {row[5]}\")\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "\n",
    "# Perform detailed sales analysis\n",
    "detailed_sales_analysis(connection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75266a00",
   "metadata": {},
   "source": [
    "## Code Chanllenge ETL on Python\n",
    "### ETL Code Challenge Description\n",
    "#### Title: ETL Process Simulation for Tech Company Sales Data\n",
    "\n",
    "Objective:\n",
    "Develop a Python-based ETL (Extract, Transform, Load) process that integrates data from multiple sources, applies specific transformations, and then loads the transformed data into a new table. This challenge tests your ability to handle data programmatically, showcasing your skills in data manipulation, SQL integration, and Python programming.\n",
    "\n",
    "Background:\n",
    "A tech company has multiple tables storing sales and product details. The sales table records transactions including the country, product category, and sales details. The product table includes specifications like capacity and color. Your task is to extract data from these tables, apply transformations to derive new insights, and load the results into a new structured format.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Extract:\n",
    "Write a Python function to retrieve data from the existing sales and product tables. The extracted data should include country, category, product capacity, color, quantity sold, and final sales amount.\n",
    "Transform:\n",
    "Implement transformations to calculate the total revenue for each product (defined as quantity * final_sales).\n",
    "Categorize each transaction based on sales volume into 'High', 'Medium', or 'Low'.\n",
    "Load:\n",
    "Design and create a new table called transformed_sales to store the transformed data.\n",
    "Load the transformed data into this table with appropriate field names and data types.\n",
    "Expected Deliverables:\n",
    "\n",
    "A Python script that implements the ETL process.\n",
    "The script should include functions for connecting to a MySQL database, executing SQL queries, and handling any potential errors.\n",
    "Documentation within the script explaining the purpose and functionality of each part of the code.\n",
    "Evaluation Criteria:\n",
    "\n",
    "Correctness: The script should correctly execute all steps of the ETL process without errors.\n",
    "Efficiency: Code and queries should be optimized for performance, especially when handling large datasets.\n",
    "Code Quality: The code should be well-organized, properly commented, and easy to read.\n",
    "Error Handling: The script should include robust error handling to manage and log potential issues during the database operations.\n",
    "Instructions for Execution:\n",
    "\n",
    "Just use mysql local community server and a made up data set related to sales of devices in a tech company and tables created on SQL portion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d98b1",
   "metadata": {},
   "source": [
    "Overview of the ETL Process\n",
    "Here's how we can structure the ETL process for your dataset:\n",
    "\n",
    "Extract: Retrieve data from the sales and product tables.\n",
    "Transform: Apply transformations to the data, such as computing additional metrics or modifying the format.\n",
    "Load: Load the transformed data into a new table or update the existing tables.\n",
    "1. Extract Data\n",
    "First, extract data from the MySQL database using the previously established connection and query functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47189c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch data from SQL\n",
    "def fetch_query(connection, query):\n",
    "    cursor = connection.cursor(dictionary=True)\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "        return result\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "        return None\n",
    "    \n",
    "def extract_data(connection):\n",
    "    sales_query = \"\"\"\n",
    "    SELECT\n",
    "        s.country,\n",
    "        s.category,\n",
    "        p.product_name,\n",
    "        p.specifications,\n",
    "        s.quantity,\n",
    "        s.final_sales\n",
    "    FROM\n",
    "        sales s\n",
    "    JOIN\n",
    "        product p ON s.category = p.category;\n",
    "    \"\"\"\n",
    "    sales_data = fetch_query(connection, sales_query)\n",
    "    return sales_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c928106",
   "metadata": {},
   "source": [
    "2. Transform Data\n",
    "We will create a simple transformation function that, for example, calculates the total revenue per product and categorizes sales based on volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(sales_data):\n",
    "    transformed_data = []\n",
    "    for row in sales_data:\n",
    "        total_revenue = row['quantity'] * row['final_sales']\n",
    "        if total_revenue > 1000:\n",
    "            sales_volume = 'High'\n",
    "        elif total_revenue > 500:\n",
    "            sales_volume = 'Medium'\n",
    "        else:\n",
    "            sales_volume = 'Low'\n",
    "        \n",
    "        transformed_row = {\n",
    "            \"country\": row['country'],\n",
    "            \"category\": row['category'],\n",
    "            \"product_name\": row['product_name'],\n",
    "            \"specifications\": row['specifications'],\n",
    "            \"quantity_sold\": row['quantity'],\n",
    "            \"total_revenue\": total_revenue,\n",
    "            \"sales_volume\": sales_volume\n",
    "        }\n",
    "        transformed_data.append(transformed_row)\n",
    "    return transformed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b0279",
   "metadata": {},
   "source": [
    "3. Load Data\n",
    "Finally, write the transformed data back into a new table or an existing one. Here, let's assume we are creating a new table to store these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221225d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(connection, transformed_data):\n",
    "    create_transformed_sales_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS transformed_sales (\n",
    "        id INT AUTO_INCREMENT,\n",
    "        country VARCHAR(255) NOT NULL,\n",
    "        category VARCHAR(255) NOT NULL,\n",
    "        product_name VARCHAR(255) NOT NULL,\n",
    "        specifications TEXT NOT NULL,\n",
    "        quantity_sold INT NOT NULL,\n",
    "        total_revenue DECIMAL(10, 2) NOT NULL,\n",
    "        sales_volume VARCHAR(50) NOT NULL,\n",
    "        PRIMARY KEY (id)\n",
    "    );\n",
    "    \"\"\"\n",
    "    execute_query(connection, create_transformed_sales_table)\n",
    "    \n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO transformed_sales (country, category, product_name, specifications, quantity_sold, total_revenue, sales_volume)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s);\n",
    "    \"\"\"\n",
    "    cursor = connection.cursor()\n",
    "    for row in transformed_data:\n",
    "        cursor.execute(insert_query, (row['country'], row['category'], row['product_name'], row['specifications'], row['quantity_sold'], row['total_revenue'], row['sales_volume']))\n",
    "    connection.commit()\n",
    "    print(\"Transformed data loaded into transformed_sales table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dbc3af",
   "metadata": {},
   "source": [
    "Execution of ETL Process\n",
    "Now, combine these functions to perform the complete ETL process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba56b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_etl_process():\n",
    "    # Define the database credentials\n",
    "    host = \"localhost\"\n",
    "    user = \"root\"\n",
    "    password = \"25789Mysql.\"\n",
    "    database = \"sales_db\"\n",
    "\n",
    "    # Create a database connection\n",
    "    connection = create_connection(host, user, password, database)\n",
    "    \n",
    "    if connection:\n",
    "        # Extract data\n",
    "        sales_data = extract_data(connection)\n",
    "        \n",
    "        # Transform data\n",
    "        transformed_data = transform_data(sales_data)\n",
    "        \n",
    "        # Load data\n",
    "        load_data(connection, transformed_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_etl_process()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ac59e",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "This simulated ETL process in Python effectively demonstrates how to extract data from a relational database, apply meaningful transformations, and then load the processed data into a new storage system, providing practical hands-on experience with ETL concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf1e16",
   "metadata": {},
   "source": [
    "## Code Chanllenge Airflow\n",
    "\n",
    "Title: Developing an Airflow DAG for an Automated ETL Process\n",
    "\n",
    "Objective:\n",
    "The goal of this challenge is to develop a fully functional Airflow Directed Acyclic Graph (DAG) that orchestrates an ETL (Extract, Transform, Load) process. This process involves extracting data from a source, transforming this data, and loading it into a destination system.\n",
    "\n",
    "Background:\n",
    "Automating ETL tasks is crucial for ensuring data accuracy and availability in real-time or near-real-time for analysis and decision-making. Airflow is a platform used to programmatically author, schedule, and monitor workflows.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Complete the Python Functions:\n",
    "Extract Function: Implement logic to extract data from a predefined data source. This could be a database, a file, an API, or any simulated data source.\n",
    "Transform Function: Apply necessary data transformations which could include cleaning, aggregating, or any other form of data manipulation.\n",
    "Load Function: Implement the logic to load the transformed data into a specified target, which could be a database or a data warehouse.\n",
    "Integrate Functions with Airflow:\n",
    "Use the provided Airflow DAG skeleton to integrate your Python functions.\n",
    "Configure the DAG to ensure that tasks are executed in the correct order, handling dependencies correctly.\n",
    "Expected Deliverables:\n",
    "\n",
    "A fully implemented Airflow DAG named etl_process_dag with the specified tasks (extract, transform, load).\n",
    "Detailed documentation on:\n",
    "The data source and data format expected.\n",
    "The specific transformations applied.\n",
    "The destination system and data schema.\n",
    "Evaluation Criteria:\n",
    "\n",
    "Functionality: The DAG should execute without errors, and data should flow through the ETL process as intended.\n",
    "Code Quality: Code should be clean, well-commented, and follow best practices for Python and Airflow.\n",
    "Error Handling: Adequate error handling should be in place to manage common failures in data extraction, transformation, and loading.\n",
    "Scalability and Maintainability: The solution should be scalable and easy to maintain or modify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "def extract_data():\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            database='sales_db',\n",
    "            user='root',\n",
    "            password='25789Mysql.'\n",
    "        )\n",
    "        if connection.is_connected():\n",
    "            cursor = connection.cursor(dictionary=True)\n",
    "            cursor.execute(\"SELECT * FROM sales\")\n",
    "            sales_data = cursor.fetchall()\n",
    "            cursor.execute(\"SELECT * FROM product\")\n",
    "            product_data = cursor.fetchall()\n",
    "        return sales_data, product_data\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            cursor.close()\n",
    "            connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f92b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(sales_data, product_data):\n",
    "    transformed_data = []\n",
    "    for sale in sales_data:\n",
    "        for product in product_data:\n",
    "            if sale['category'] == product['category']:\n",
    "                total_revenue = sale['quantity'] * sale['final_sales']\n",
    "                if total_revenue > 1000:\n",
    "                    sales_volume = 'High'\n",
    "                elif total_revenue > 500:\n",
    "                    sales_volume = 'Medium'\n",
    "                else:\n",
    "                    sales_volume = 'Low'\n",
    "                \n",
    "                transformed_row = {\n",
    "                    \"country\": sale['country'],\n",
    "                    \"category\": sale['category'],\n",
    "                    \"product_name\": product['product_name'],\n",
    "                    \"specifications\": product['specifications'],\n",
    "                    \"quantity_sold\": sale['quantity'],\n",
    "                    \"total_revenue\": total_revenue,\n",
    "                    \"sales_volume\": sales_volume\n",
    "                }\n",
    "                transformed_data.append(transformed_row)\n",
    "    return transformed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce56928",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apache-airflow==2.0.0\n",
    "!pip install apache-airflow[postgres, mysql]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f5b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "# We define the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'etl_process_dag',\n",
    "    default_args=default_args,\n",
    "    description='A simple ETL DAG',\n",
    "    schedule_interval='@daily',\n",
    "    start_date=days_ago(1),\n",
    ")\n",
    "\n",
    "# We define the tasks using PythonOperator\n",
    "def extract_task(**kwargs):\n",
    "    sales_data, product_data = extract_data()\n",
    "    kwargs['ti'].xcom_push(key='sales_data', value=sales_data)\n",
    "    kwargs['ti'].xcom_push(key='product_data', value=product_data)\n",
    "\n",
    "def transform_task(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    sales_data = ti.xcom_pull(key='sales_data', task_ids='extract')\n",
    "    product_data = ti.xcom_pull(key='product_data', task_ids='extract')\n",
    "    transformed_data = transform_data(sales_data, product_data)\n",
    "    ti.xcom_push(key='transformed_data', value=transformed_data)\n",
    "\n",
    "def load_task(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    transformed_data = ti.xcom_pull(key='transformed_data', task_ids='transform')\n",
    "    load_data(transformed_data)\n",
    "\n",
    "extract = PythonOperator(\n",
    "    task_id='extract',\n",
    "    python_callable=extract_task,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "transform = PythonOperator(\n",
    "    task_id='transform',\n",
    "    python_callable=transform_task,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "load = PythonOperator(\n",
    "    task_id='load',\n",
    "    python_callable=load_task,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# We define the dependencies\n",
    "extract >> transform >> load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb7495",
   "metadata": {},
   "source": [
    "## Code Chanllenge API\n",
    "\n",
    "API Data Integration Code Challenge Description\n",
    "Title: API Data Handling and Integration Challenge\n",
    "\n",
    "Objective:\n",
    "Develop a Python application that fetches data from an external API, applies specified transformations, and outputs the processed data. This challenge is designed to test your abilities in API interaction, data manipulation, and the application of basic data processing principles in Python.\n",
    "\n",
    "Background:\n",
    "APIs are a crucial data source in many software systems and data pipelines. Effective handling and integration of API data are key skills for developers and data engineers, involving tasks such as data extraction, transformation, and preparation for further analysis or storage.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Extract Data:\n",
    "Write a Python function to fetch data from a given API endpoint. This function should handle network errors, API rate limits, and other common issues that can occur during API interaction.\n",
    "Transform Data:\n",
    "Implement logic to transform the raw data fetched from the API. Assume the data includes various product details; extract and format this data into a structured JSON format that focuses on specific fields like product_id, product_name, category, and price.\n",
    "Output Data:\n",
    "Instead of loading the data into a database or storage system, output the transformed data to the console or a file in a clean, readable format. This simulates the final step in an ETL process where data is made available for further use.\n",
    "Expected Deliverables:\n",
    "\n",
    "A Python script that efficiently and correctly extracts, transforms, and outputs data as described.\n",
    "Effective use of exception handling to manage potential errors during the API request.\n",
    "Logging throughout the process to track operations and facilitate debugging and monitoring.\n",
    "Evaluation Criteria:\n",
    "\n",
    "Correctness and Completeness: The script should correctly fetch and process the API data according to the specifications provided.\n",
    "Error Handling: Robust handling of errors and exceptional conditions in the API interaction.\n",
    "Code Quality: The code should be clean, well-organized, commented, and follow best practices for Python development.\n",
    "Output Formatting: The transformed data should be outputted in a structured and readable format, demonstrating an understanding of data presentation.\n",
    "Instructions for Setup and Execution:\n",
    "\n",
    "Ensure the requests and logging libraries are installed in your Python environment.\n",
    "Use any public API endpoint of your preference if tech_company related would be better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf313080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Función para extraer datos de la API\n",
    "def fetch_api_data(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        data = response.json()\n",
    "        logging.info(\"Data fetched successfully from API.\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching data from API: {e}\")\n",
    "        return None\n",
    "\n",
    "# Función para transformar datos\n",
    "def transform_data(data):\n",
    "    transformed_data = []\n",
    "    for item in data:\n",
    "        transformed_item = {\n",
    "            \"product_id\": item.get(\"id\"),\n",
    "            \"product_name\": item.get(\"name\"),\n",
    "            \"category\": item.get(\"category\"),\n",
    "            \"price\": item.get(\"price\")\n",
    "        }\n",
    "        transformed_data.append(transformed_item)\n",
    "    logging.info(\"Data transformed successfully.\")\n",
    "    return transformed_data\n",
    "\n",
    "# Función para salida de datos\n",
    "def output_data(data, output_file):\n",
    "    try:\n",
    "        with open(output_file, 'w') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "        logging.info(f\"Data successfully written to {output_file}\")\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Error writing data to file: {e}\")\n",
    "\n",
    "# Función principal\n",
    "def main():\n",
    "    api_url = \"https://fakestoreapi.com/products\"  # URL de ejemplo de API pública\n",
    "    output_file = \"transformed_data.json\"\n",
    "\n",
    "    # Extraer datos de la API\n",
    "    raw_data = fetch_api_data(api_url)\n",
    "    if raw_data:\n",
    "        # Transformar datos\n",
    "        transformed_data = transform_data(raw_data)\n",
    "        # Salida de datos\n",
    "        output_data(transformed_data, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d36dd0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
